{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Corpus, Tokens and N-Grams**\n",
    "\n",
    "Corpus > Documents > Paragraphs > Sentences > Tokens\n",
    "\n",
    "\n",
    "* **Corpus**: Collection of text documents\n",
    "* **Tokens**: Smaller units of a text (words, phrases, ngrams)\n",
    "* **N-Grams**: combinations of N words / Characters together\n",
    "\n",
    "\n",
    "* **Unigrams**: I, Love, My, Phone\n",
    "* **Bigrams**: I Love, Love my, My phone\n",
    "* **Trigrams**: I love my, love my phone"
   ],
   "id": "afa17ac5ff0e7fff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T13:18:39.502341Z",
     "start_time": "2024-10-07T13:18:39.493821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "print(wordnet.synsets('good'))\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize,  word_tokenize\n",
    "\n",
    "sentence = \"\\nI love to play football\"\n",
    "print (sentence)\n",
    "\n",
    "n = 2\n",
    "for gram in ngrams(word_tokenize(sentence), n):\n",
    "    print(gram)"
   ],
   "id": "6543b125c5ea60dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('good.n.01'), Synset('good.n.02'), Synset('good.n.03'), Synset('commodity.n.01'), Synset('good.a.01'), Synset('full.s.06'), Synset('good.a.03'), Synset('estimable.s.02'), Synset('beneficial.s.01'), Synset('good.s.06'), Synset('good.s.07'), Synset('adept.s.01'), Synset('good.s.09'), Synset('dear.s.02'), Synset('dependable.s.04'), Synset('good.s.12'), Synset('good.s.13'), Synset('effective.s.04'), Synset('good.s.15'), Synset('good.s.16'), Synset('good.s.17'), Synset('good.s.18'), Synset('good.s.19'), Synset('good.s.20'), Synset('good.s.21'), Synset('well.r.01'), Synset('thoroughly.r.02')]\n",
      "\n",
      "I love to play football\n",
      "('I', 'love')\n",
      "('love', 'to')\n",
      "('to', 'play')\n",
      "('play', 'football')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hikari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Tokenization**\n",
    "\n",
    "Process of splittiong a text object into smaller units (tokens)\n",
    "\n",
    "Smaller Units: words, numbers, symbols, ngrams, characters\n",
    "\n",
    "White space tokenizer/ Unigram tokenizer\n",
    "\n",
    "* **Sentence:** \"I went to New-York to play footbal\"\n",
    "* **Tokens:** \"I\", \"went\", \"to\", \"New-York\", \"to\", \"play\", \"footbal\"\n",
    "\n",
    "Regular expression tokenizer\n",
    "\n",
    "* **Sentence:** \"Footbal, Cricket; Golf Tennis\"\n",
    "* **Tokens:** \"Footbal\", \"Cricket\", \"Golf\", \"Tennis\""
   ],
   "id": "bc2ca12f95a94ecc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T13:10:56.507116Z",
     "start_time": "2024-10-07T13:10:56.497666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize,  word_tokenize\n",
    "text = 'Hi John, How are you doing? I will be travelling to your city. Lets catchup'\n",
    "\n",
    "print(sent_tokenize(text))\n",
    "print(word_tokenize(text))"
   ],
   "id": "5627a7fb747bc063",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi John, How are you doing?', 'I will be travelling to your city.', 'Lets catchup']\n",
      "['Hi', 'John', ',', 'How', 'are', 'you', 'doing', '?', 'I', 'will', 'be', 'travelling', 'to', 'your', 'city', '.', 'Lets', 'catchup']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hikari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Normalization**\n",
    "\n",
    "**Morpheme:** base form of a word\n",
    "\n",
    "**Structure of token**: <prefix><morpheme><suffix>\n",
    "\n",
    "* **Example:** Antinationalist = Anti+national+ist\n",
    "\n",
    "**Normalization:** Process of converting a token into its base form (morpheme)\n",
    "\n",
    "Helpful in reducing data dimensionality, text cleaning\n",
    "\n",
    "**Types:** Stemming and Lemitization"
   ],
   "id": "cd9c64ca455811b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T13:11:00.231706Z",
     "start_time": "2024-10-07T13:11:00.174174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ],
   "id": "6bf041851fab5183",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hikari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Normalization: Stemming** \n",
    "\n",
    "Elementary rule based process of removal of infectional form from a token\n",
    "\n",
    "Outputs the stem of a word\n",
    "\n",
    "\"laughing\", \"laughed\", \"laughs\", \"laugh\" >>> \"laugh\"\n",
    "\n",
    "Form Suffix Stem\n",
    "studies es studi\n",
    "studying ing study\n",
    "\n",
    "\n",
    "* **Example:** \n",
    "* his teams are not winning\n",
    "* hi team are not winn"
   ],
   "id": "54b9f6e2686f36a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T13:11:03.043752Z",
     "start_time": "2024-10-07T13:11:03.034661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()   \n",
    "\n",
    "print(stemmer.stem(\"player\"))\n",
    "print(stemmer.stem(\"playing\"))\n",
    "print(stemmer.stem(\"plays\"))\n",
    "print(stemmer.stem(\"increases\"))\n"
   ],
   "id": "6935d9486e5a8f55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player\n",
      "play\n",
      "play\n",
      "increas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hikari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Normalization: Lemmatization**\n",
    "\n",
    "Systematic process for reducing a token to its lemma\n",
    "\n",
    "Makes use of vocabulary, word structure, part of speech tags and grammar relations\n",
    "\n",
    "**Example:** \n",
    "\n",
    "* am,are,is >> be\n",
    "* running, ran, run, rans >> run\n",
    "\n",
    "**verb** Running >> run\n",
    "\n",
    "**noun** Running >> running\n",
    "\n",
    "**Multiplicate word**\n",
    "\n",
    "* multiplications>>multiplication>>multiplicate>>multiple\n",
    "* Multiplicati(vely)(vily) >> multiplicative >> multiplicate\n",
    "* Multiplicably >> Multiplicable >> Multiplicate >>Multiole\n",
    "* Multiples >> Multiple\n",
    "* Multiplied >> Multiply >> Multiple\n",
    "* Multipliers >> Multiplier >> Multiply >> Multiple\n",
    "* Multiplies >> Multiply >> Multiple\n",
    "* Multiplying >> Multiply >> Multiple\n",
    "* Multipliably >> Multipliable >> Multiply >> Multiple"
   ],
   "id": "da6b38b69d295f04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T13:11:07.649878Z",
     "start_time": "2024-10-07T13:11:07.643136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "print(lemm.lemmatize(\"increases\"))\n",
    "print(lemm.lemmatize(\"running\", pos=\"v\"))"
   ],
   "id": "a3c086bb1c4a2dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase\n",
      "run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hikari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Part of speech tags**\n",
    "\n",
    "Defines the syntactic context and role of words in the sentence\n",
    "\n",
    "**Common POS Tags:** Noun, Verb, Adjectives, Adverbs\n",
    "\n",
    "**Sentence:** David has purchased a new Laptop from Apple Store\n",
    "\n",
    "* {David, NNP}\n",
    "* {has, VBZ}\n",
    "* {purchased, VBN}\n",
    "* {a, DT}\n",
    "* {new, JJ}\n",
    "* {laptop, NN}\n",
    "* {from, IN}\n",
    "* {Apple, NNP}\n",
    "* {store, NN}\n",
    "\n",
    "Defined by their relationship with the adjacent words\n",
    "\n",
    "**English tag set**\n",
    "\n",
    "| **Tag** | **Description**                                            | **Example**                                                     |\n",
    "|---------|------------------------------------------------------------|-----------------------------------------------------------------|\n",
    "| DT      | Determiner                                                 | the, a, an, this, that, these, those                            |\n",
    "| QT      | Quantifier                                                 | some, any, many, few, several, enough                           |\n",
    "| CD      | Cardinal number                                            | one, two, three, four, five, ...                                |\n",
    "| NN      | Noun, singular                                             | book, table, dog, city                                          |\n",
    "| NNS     | Noun, plural                                               | books, tables, dogs, cities                                     |\n",
    "| NNP     | Proper noun, singular                                      | John, Mary, New York                                            |\n",
    "| NNPS    | Proper noun, plural                                        | United States, Beatles                                          |\n",
    "| EX      | Existential there                                          | There was a party.                                              |\n",
    "| PRP     | Personal pronoun                                           | I, you, he, she, it, we, they, me, <br/>you, him, her, us, them |\n",
    "| PRP$    | Possessive pronoun                                         | my, your, his, her, <br/>its, our, their                        |\n",
    "| POS     | Possessive ending                                          | 's, '                                                           |\n",
    "| RBS     | Adverb, superlative                                        | most, best, worst                                               |\n",
    "| RBR     | Adverb, comparative                                        | more, better, worse                                             |\n",
    "| RB      | Adverb                                                     | quickly, happily, very, often                                   |\n",
    "| JJS     | Adjective, superlative                                     | biggest, best, worst                                            |\n",
    "| JJR     | Adjective, comparative                                     | bigger, better, worse                                           |\n",
    "| JJ      | Adjective                                                  | big, good, bad                                                  |\n",
    "| MD      | Modal                                                      | can, could, may, might, <br/>must, should, will, would          |\n",
    "| VB      | Verb, base form                                            | go, eat, play                                                   |\n",
    "| VBP     | Verb, present tense, <br/>other than third person singular | go,                                                             |\n",
    "| VBZ     | Verb, present tense, <br/>third person singular            | goes                                                            |\n",
    "| VBD     | Verb, past tense                                           | went, ate, played                                               |\n",
    "| VBN     | Verb, past participle                                      | gone, eaten, played                                             |\n",
    "| VBG     | Verb, gerund or present participle                         | going, eating, playing                                          |\n",
    "| WDT     | Wh-determiner                                              | which, what,<br/> where, when,<br/> who, whose                  |\n",
    "\n",
    "| **Tag**  | **Description**  | **Example** |\n",
    "|----------|------------------|-------------|\n",
    "| WP       | Wh-pronoun        | who, whom, which, what |\n",
    "| WP$      | Possessive wh-pronoun | whose |\n",
    "| WRB      | Wh-adverb         | when, where, why, how |\n",
    "| TO       | The preposition to | to go, to eat, to play |\n",
    "| IN       | Preposition or <br/>subordinating conjunction | in, on, at, for, because, since |\n",
    "| CC       | Coordinating <br/>conjunction | and, but, or, nor, for, yet, so |\n",
    "| UH       | Interjection      | oh, wow, ouch |\n",
    "| RP       | Particle          | up, down, out, off |\n",
    "| SYM      | Symbol            | , |\n",
    "| \\        | Currency sign     | \\ |\n",
    "| ''       | Double or single<br/> quotation marks | \"hello\", 'world' |\n",
    "| (        | Opening parenthesis,<br/> bracket,<br/> angle bracket, or brace | ( |\n",
    "| )        | Closing parenthesis,<br/> bracket,<br/> angle bracket, or brace | ) |\n",
    "| ,        | Comma             | , |\n",
    "| .        | End of sentence punctuation | . ! ? |\n",
    "| :        | Mid-sentence punctuation | : ; ... -- - |"
   ],
   "id": "1ceabe82a96cc28b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T13:11:12.679589Z",
     "start_time": "2024-10-07T13:11:12.525556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk import pos_tag, ngrams\n",
    "\n",
    "text = 'Hi John, How are you doing? I will be travelling to your city. Lets catchup'\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)"
   ],
   "id": "884a601b35cf8942",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'NNP'), ('John', 'NNP'), (',', ','), ('How', 'NNP'), ('are', 'VBP'), ('you', 'PRP'), ('doing', 'VBG'), ('?', '.'), ('I', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('travelling', 'VBG'), ('to', 'TO'), ('your', 'PRP$'), ('city', 'NN'), ('.', '.'), ('Lets', 'VBZ'), ('catchup', 'JJ')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Hikari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Constituency Grammar**\n",
    "\n",
    "**Constituents:** Words / phrases / group of words\n",
    "\n",
    "**Constituency Grammar:** Organize any sentence into constituents using their properties\n",
    "\n",
    "**Properties:** Part of Speech Tags / Noun Phrases / Verb Phrases\n",
    "\n",
    "**Sentence:** \\<subject\\>\\<context\\>\\<object\\>\n",
    "\n",
    "* **<subject>** The cats / The dogs / They\n",
    "* **<context>** are running / are barking / are eating\n",
    "* **<object>** in the park / happily / since the morning\n",
    "\n",
    "**Another view (using part of speech)**\n",
    "\n",
    "\\<DT NN\\> \\<JJ VB\\> \\<PRP DT NN\\> ---> The dogs are barking in the park"
   ],
   "id": "f0cd1ddbf150ac1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T13:11:14.974272Z",
     "start_time": "2024-10-07T13:11:14.962190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk import CFG\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> DT NN | DT NNS | PRP\n",
    "  VP -> VBZ VP | VBG PP | VBG Adv\n",
    "  PP -> IN NP\n",
    "  Adv -> RB\n",
    "  DT -> 'The' | 'the'\n",
    "  NN -> 'dog' | 'cat' | 'park'\n",
    "  NNS -> 'dogs' | 'cats'\n",
    "  PRP -> 'They'\n",
    "  VBZ -> 'are'\n",
    "  VBG -> 'running' | 'barking' | 'eating'\n",
    "  IN -> 'in'\n",
    "  RB -> 'happily'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "\n",
    "sentence = \"The dogs are barking in the park\".split()\n",
    "\n",
    "\n",
    "for tree in parser.parse(sentence):\n",
    "    #print(tree)\n",
    "    tree.pretty_print()\n",
    "\n"
   ],
   "id": "7a74c25e54f1553c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    S                        \n",
      "      ______________|_____                    \n",
      "     |                    VP                 \n",
      "     |         ___________|___                \n",
      "     |        |               VP             \n",
      "     |        |      _________|___            \n",
      "     |        |     |             PP         \n",
      "     |        |     |      _______|___        \n",
      "     NP       |     |     |           NP     \n",
      "  ___|___     |     |     |        ___|___    \n",
      " DT     NNS  VBZ   VBG    IN      DT      NN \n",
      " |       |    |     |     |       |       |   \n",
      "The     dogs are barking  in     the     park\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hikari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Hikari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Dependency Grammar**\n",
    "\n",
    "Words of a sentence depends on which other word (dependencies)\n",
    "\n",
    "* **Example:** Modifiers (barking dog)\n",
    "\n",
    "Organize words a sentence according to their dependencies\n",
    "\n",
    "All the words are directly or indirectly linked to the root using links\n",
    "\n",
    "These dependencies represents relationships among the words in a sentence \n",
    "\n",
    "**Sentence:** User is the largest community of data scientist and provides best resources for \n",
    "understanding data and analytics \n",
    "\n",
    "* **nsubj:** {User, NNP} and {community, NN}\n",
    "* **cop:** {is, VBZ} and {community, NN}\n",
    "* **det:** {the, DT} and {community, NN}\n",
    "* **amod:** {largest, JJS} and {community, NN}\n",
    "* **nmod:**  {community, NN} and { scientist , NNS }\n",
    "* **case:**  {of, IN} and { scientist , NNS }\n",
    "* **compound:**  {data, NNS} and { scientist , NNS }\n",
    "* **cc:**  {community, NN} and { and , CC }\n",
    "* **conj:**  {community, NN} and { provides , VBZ }\n",
    "* **dobj:**  { provides , VBZ } and { resouces , NNS }\n",
    "* **amod:**  { best , JJS } and { resouces , NNS }\n",
    "* **acl:**  { resouces , NNS } and { understanding , VBG }  \n",
    "* **mark:**  { for , IN } and { understanding , VBG }  \n",
    "* **dobj:**  { understanding , VBG } and  { data , NNS }\n",
    "* **cc:**  { data , NNS } and  { and , CC}\n",
    "* **conj:**  { data , NNS } and  { analytics , NNS}\n",
    "\n",
    "Relation: (Governer, Relation, Dependent)\n",
    "\n",
    "* \\<User\\>\\<is\\>\\<the largest community of data scientists\\>"
   ],
   "id": "bb718718db8c2964"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T13:20:16.858843Z",
     "start_time": "2024-10-07T13:20:16.342961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentence = \"User is the largest community of data scientists and provides best resources for understanding data and analytics.\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text:10} {token.dep_:10} {token.head.text:10} {token.head.pos_:10} {token.pos_:10}')\n",
    "\n",
    "\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)\n"
   ],
   "id": "994b7da6b26f3b94",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m displacy\n\u001B[0;32m      4\u001B[0m nlp \u001B[38;5;241m=\u001B[39m spacy\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124men_core_web_sm\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Dict, Iterable, Union\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m setup_default_warnings\n\u001B[0;32m      8\u001B[0m setup_default_warnings()  \u001B[38;5;66;03m# noqa: E402\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# These are imported as part of the API\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\errors.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Literal\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mErrorsWithCodes\u001B[39;00m(\u001B[38;5;28mtype\u001B[39m):\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattribute__\u001B[39m(\u001B[38;5;28mself\u001B[39m, code):\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\compat.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mthinc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m copy_array\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcPickle\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\__init__.py:5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabout\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m registry\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# fmt: off\u001B[39;00m\n\u001B[0;32m      8\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregistry\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__version__\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     11\u001B[0m ]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\config.py:5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mconfection\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mconfection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Decorator\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mregistry\u001B[39;00m(confection\u001B[38;5;241m.\u001B[39mregistry):\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# fmt: off\u001B[39;00m\n\u001B[0;32m     10\u001B[0m     optimizers: Decorator \u001B[38;5;241m=\u001B[39m catalogue\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthinc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimizers\u001B[39m\u001B[38;5;124m\"\u001B[39m, entry_points\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\types.py:25\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      5\u001B[0m     Any,\n\u001B[0;32m      6\u001B[0m     Callable,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     20\u001B[0m     overload,\n\u001B[0;32m     21\u001B[0m )\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cupy, has_cupy\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_cupy:\n\u001B[0;32m     28\u001B[0m     get_array_module \u001B[38;5;241m=\u001B[39m cupy\u001B[38;5;241m.\u001B[39mget_array_module\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\compat.py:99\u001B[0m\n\u001B[0;32m     95\u001B[0m has_mxnet \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 99\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mh5py\u001B[39;00m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[0;32m    101\u001B[0m     h5py \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py:45\u001B[0m\n\u001B[0;32m     36\u001B[0m     _warn((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh5py is running against HDF5 \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m when it was built against \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     37\u001B[0m            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthis may cause problems\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m     38\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;241m*\u001B[39mversion\u001B[38;5;241m.\u001B[39mhdf5_version_tuple),\n\u001B[0;32m     39\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;241m*\u001B[39mversion\u001B[38;5;241m.\u001B[39mhdf5_built_version_tuple)\n\u001B[0;32m     40\u001B[0m     ))\n\u001B[0;32m     43\u001B[0m _errors\u001B[38;5;241m.\u001B[39msilence_errors()\n\u001B[1;32m---> 45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_conv\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m register_converters \u001B[38;5;28;01mas\u001B[39;00m _register_converters, \\\n\u001B[0;32m     46\u001B[0m                    unregister_converters \u001B[38;5;28;01mas\u001B[39;00m _unregister_converters\n\u001B[0;32m     47\u001B[0m _register_converters()\n\u001B[0;32m     48\u001B[0m atexit\u001B[38;5;241m.\u001B[39mregister(_unregister_converters)\n",
      "File \u001B[1;32mh5py\\\\_conv.pyx:1\u001B[0m, in \u001B[0;36minit h5py._conv\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mh5py\\\\h5r.pyx:1\u001B[0m, in \u001B[0;36minit h5py.h5r\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mh5py\\\\h5p.pyx:1\u001B[0m, in \u001B[0;36minit h5py.h5p\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dependency Grammar - Use Cases\n",
    "\n",
    "* Named Entity Recognition\n",
    "* Question Answering Systems\n",
    "* Coreference Resolution\n",
    "* Text Summarization\n",
    "* Text Classifications"
   ],
   "id": "9950a2d5dc65e179"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T05:14:39.073114Z",
     "start_time": "2024-10-07T05:14:39.069283Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8283e4a68788affb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
